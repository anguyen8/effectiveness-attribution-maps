{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Visualize explanations for all test trials for all tested method\n",
    "2. Run AI-only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M5Cpmdbql1Gp"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KRNH8n6pl1Gz",
    "outputId": "76fc64f7-9cdd-4367-f881-581d87918e27"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import models\n",
    "from torchvision import transforms\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "torch.set_num_threads(1)\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "from torchvision.models import *\n",
    "from visualisation.core.utils import device, image_net_postprocessing\n",
    "from torch import nn\n",
    "from operator import itemgetter\n",
    "from visualisation.core.utils import imshow\n",
    "from IPython.core.debugger import Tracer\n",
    "\n",
    "NN_flag = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1s51UO4ul1HC",
    "outputId": "1981877b-002c-4932-a1ee-3d38a98089b8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (4): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (5): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer = 4\n",
    "if NN_flag:\n",
    "    feature_extractor = nn.Sequential(*list(resnet34(pretrained=True).children())[:layer-6]).to(device)\n",
    "    \n",
    "model = resnet34(pretrained=True).to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(feature_extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6qQeP4Aml1HH"
   },
   "outputs": [],
   "source": [
    "# %matplotlib notebook \n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch \n",
    "from utils import *\n",
    "from PIL import Image\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"]= 16,8\n",
    "\n",
    "\n",
    "def make_dir(path):\n",
    "    if os.path.isdir(path) == False:\n",
    "        os.mkdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WwZvXYWFl1HR"
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.pyplot import imshow\n",
    "\n",
    "from visualisation.core.utils import device \n",
    "from PIL import Image\n",
    "from torchvision.transforms import ToTensor, Resize, Compose, ToPILImage\n",
    "from visualisation.core import *\n",
    "from visualisation.core.utils import image_net_preprocessing\n",
    "\n",
    "size = 224\n",
    "\n",
    "# Pre-process the image and convert into a tensor\n",
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize(256),\n",
    "    torchvision.transforms.CenterCrop(224),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "img_num = 50\n",
    "\n",
    "# methods = ['Conf', 'GradCAM', 'EP', 'SHAP', 'NNs', 'PoolNet', 'AIonly']\n",
    "methods = ['Conf', 'GradCAM', 'EP', 'NNs', 'PoolNet', 'AIonly']\n",
    "\n",
    "task = 'Natural'\n",
    "# Adversarial_Nat\n",
    "\n",
    "output_path = '/home/dexter/Downloads/Human_experiments/Visualization'\n",
    "dataset_path = '/home/dexter/Downloads/Human_experiments/Dataset'\n",
    "make_dir('{}/{}'.format(output_path,task))\n",
    "make_dir('{}/{}/Conf/'.format(output_path,task))\n",
    "make_dir('{}/{}/GradCAM/'.format(output_path,task))\n",
    "make_dir('{}/{}/EP/'.format(output_path,task))\n",
    "# make_dir('Pilot_study2/{}/SHAP/'.format(task))\n",
    "make_dir('{}/{}/PoolNet/'.format(output_path,task))\n",
    "make_dir('{}/{}/NNs/'.format(output_path,task))\n",
    "# test_image_paths = glob.glob('/home/dexter/Downloads/Pilot_study2/{}/mixed_images/*.*'.format(task))\n",
    "corrects_bin1 = glob.glob('{}/{}/correct_bin1_images/*.*'.format(dataset_path,task))\n",
    "wrongs_bin1 = glob.glob('{}/{}/wrong_bin1_images/*.*'.format(dataset_path,task))\n",
    "corrects_bin2 = glob.glob('{}/{}/correct_bin2_images/*.*'.format(dataset_path,task))\n",
    "wrongs_bin2 = glob.glob('{}/{}/wrong_bin2_images/*.*'.format(dataset_path,task))\n",
    "corrects_bin3 = glob.glob('{}/{}/correct_bin3_images/*.*'.format(dataset_path,task))\n",
    "wrongs_bin3 = glob.glob('{}/{}/wrong_bin3_images/*.*'.format(dataset_path,task))\n",
    "# print(test_image_paths)\n",
    "# test_inputs = list()\n",
    "test_images = list()\n",
    "\n",
    "modified_img = list()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HQ2SjgvMl1IJ"
   },
   "outputs": [],
   "source": [
    "# Added for loading ImageNet classes\n",
    "def load_imagenet_label_map():\n",
    "    \"\"\"\n",
    "    Load ImageNet label dictionary.\n",
    "    return:\n",
    "    \"\"\"\n",
    "\n",
    "    input_f = open(\"input_txt_files/imagenet_classes.txt\")\n",
    "    label_map = {}\n",
    "    for line in input_f:\n",
    "        parts = line.strip().split(\": \")\n",
    "        (num, label) = (int(parts[0]), parts[1].replace('\"', \"\"))\n",
    "        label_map[num] = label\n",
    "\n",
    "    input_f.close()\n",
    "    return label_map\n",
    "\n",
    "\n",
    "# Added for loading ImageNet classes\n",
    "def load_imagenet_id_map():\n",
    "    \"\"\"\n",
    "    Load ImageNet ID dictionary.\n",
    "    return;\n",
    "    \"\"\"\n",
    "\n",
    "    input_f = open(\"input_txt_files/synset_words.txt\")\n",
    "    label_map = {}\n",
    "    for line in input_f:\n",
    "        parts = line.strip().split(\" \")\n",
    "        (num, label) = (parts[0], ' '.join(parts[1:]))\n",
    "        label_map[num] = label\n",
    "\n",
    "    input_f.close()\n",
    "    return label_map\n",
    "\n",
    "def convert_imagenet_label_to_id(label_map, key_list, val_list, prediction_class):\n",
    "    \"\"\"\n",
    "    Convert imagenet label to ID: for example - 245 -> \"French bulldog\" -> n02108915\n",
    "    :param label_map:\n",
    "    :param key_list:\n",
    "    :param val_list:\n",
    "    :param prediction_class:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    class_to_label = label_map[prediction_class]\n",
    "    prediction_id = key_list[val_list.index(class_to_label)]\n",
    "    return prediction_id\n",
    "\n",
    "\n",
    "# gt_dict = load_imagenet_validation_gt()\n",
    "id_map = load_imagenet_id_map()\n",
    "label_map = load_imagenet_label_map()\n",
    "\n",
    "key_list = list(id_map.keys())\n",
    "val_list = list(id_map.values())\n",
    "\n",
    "def convert_imagenet_id_to_label(label_map, key_list, val_list, class_id):\n",
    "    \"\"\"\n",
    "    Convert imagenet label to ID: for example - n02108915 -> \"French bulldog\" -> 245\n",
    "    :param label_map:\n",
    "    :param key_list:\n",
    "    :param val_list:\n",
    "    :param prediction_class:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    return key_list.index(str(class_id))\n",
    "\n",
    "from torchray.attribution.extremal_perturbation import extremal_perturbation, contrastive_reward\n",
    "from torchray.attribution.grad_cam import grad_cam\n",
    "import PIL.Image\n",
    "\n",
    "def get_EP_saliency_maps(model, path):\n",
    "        img_index = (path.split('.jpeg')[0]).split('images/')[1]\n",
    "        img = PIL.Image.open(path)\n",
    "        x = transform(img).unsqueeze(0).to(device)\n",
    "        out = model(x)\n",
    "        p = torch.nn.functional.softmax(out, dim=1)\n",
    "        score, index = torch.topk(p, 1)\n",
    "        category_id_1 = index[0][0].item()\n",
    "        \n",
    "        gt_label_id = img_index.split('val_')[1][9:18]\n",
    "        input_prediction_id = convert_imagenet_label_to_id(label_map, key_list, val_list, category_id_1)\n",
    "\n",
    "        masks, energy = extremal_perturbation(\n",
    "            model, x, category_id_1,\n",
    "            areas=[0.025, 0.05, 0.1, 0.2],\n",
    "            num_levels=8,\n",
    "            step=7,\n",
    "            sigma=7 * 3,\n",
    "            max_iter=800,\n",
    "            debug=False,\n",
    "            jitter=True,\n",
    "            smooth=0.09,\n",
    "            perturbation='blur'\n",
    "        )\n",
    "        saliency = masks.sum(dim=0, keepdim=True)\n",
    "        saliency = saliency.detach()\n",
    "        saliency_t = saliency.cpu().detach().numpy()[0, 0, :]\n",
    "        \n",
    "        saliency_path = 'saliency_maps/EP_resnet34_{}/'.format(task)\n",
    "        if not (os.path.exists(saliency_path)):\n",
    "            os.mkdir(saliency_path)\n",
    "        \n",
    "#         # Save saliency maps only when the image is correctly classified\n",
    "#         if gt_label_id == input_prediction_id:\n",
    "#             np.save(os.path.join(saliency_path, \"{}.npy\".format(img_index)), saliency_t)\n",
    "\n",
    "        return (saliency[0].to('cpu')), (masks[0].to('cpu')), (masks[1].to('cpu')), (masks[2].to('cpu')) , (masks[3].to('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "import os.path\n",
    "from visualisation.core.utils import tensor2cam\n",
    "postprocessing_t = image_net_postprocessing\n",
    "import cv2 as cv\n",
    "import sys\n",
    "\n",
    "imagenet_train_path = '/home/dexter/Downloads/train'\n",
    "\n",
    "## Creating colormap\n",
    "cMap = 'Reds'\n",
    "\n",
    "id_list= list()\n",
    "conf_dict = {}\n",
    "eps=1e-5\n",
    "cnt = 0\n",
    "K = 3 # Change to your expected number of nearest neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = 'Conf'\n",
    "test_image_paths = corrects_bin1[methods.index(method)*img_num:methods.index(method)*img_num + img_num] + \\\n",
    "                    wrongs_bin1[methods.index(method)*img_num:methods.index(method)*img_num + img_num] + \\\n",
    "                    corrects_bin2[methods.index(method)*img_num:methods.index(method)*img_num + img_num] + \\\n",
    "                    wrongs_bin2[methods.index(method)*img_num:methods.index(method)*img_num + img_num] + \\\n",
    "                    corrects_bin3[methods.index(method)*img_num:methods.index(method)*img_num + img_num] + \\\n",
    "                    wrongs_bin3[methods.index(method)*img_num:methods.index(method)*img_num + img_num]\n",
    "\n",
    "# test_image_paths = corrects_bin1[methods.index(method)*img_num + img_num:] + \\\n",
    "#                     wrongs_bin1[methods.index(method)*img_num + img_num:] + \\\n",
    "#                     corrects_bin2[methods.index(method)*img_num + img_num:] + \\\n",
    "#                     wrongs_bin2[methods.index(method)*img_num + img_num:] + \\\n",
    "#                     corrects_bin3[methods.index(method)*img_num + img_num:] + \\\n",
    "#                     wrongs_bin3[methods.index(method)*img_num + img_num:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Fflzflfhl1Ic",
    "outputId": "1c2d595f-c863-444a-dd92-f97c4e5f3331",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for idx, image_path in enumerate(test_image_paths[methods.index(method)*img_num:methods.index(method)*img_num + img_num]):\n",
    "for idx, image_path in enumerate(test_image_paths):\n",
    "#     print(image_path)\n",
    "#     if 'n02106166_ILSVRC2012_val_00043367' not in image_path:\n",
    "#         continue\n",
    "    print(idx + 1)\n",
    "    distance_dict = dict()\n",
    "    neighbors = list()\n",
    "    categories_confidences = list()\n",
    "    confidences= list ()\n",
    "\n",
    "    img = Image.open(image_path)\n",
    "    \n",
    "    if NN_flag:\n",
    "        embedding = feature_extractor(transform(img).unsqueeze(0).to(device)).flatten(start_dim=1) \n",
    "\n",
    "    input_image = img.resize((size,size), Image.ANTIALIAS)\n",
    "    image_name = (image_path.split('.jpeg')[0]).split('images/')[1]\n",
    "    print(image_name)\n",
    "\n",
    "    # Get the ground truth of the input image\n",
    "    gt_label_id = image_path.split('val_')[1][9:18]\n",
    "\n",
    "    \n",
    "    gt_label = id_map.get(gt_label_id)\n",
    "    id = key_list.index(gt_label_id)\n",
    "    gt_label = gt_label.split(',')[0]\n",
    "\n",
    "\n",
    "\n",
    "    # Get the prediction for the input image\n",
    "    img = Image.open(image_path)\n",
    "    x = transform(img).unsqueeze(0).to(device)\n",
    "    out = model(x)\n",
    "    p = torch.nn.functional.softmax(out, dim=1)\n",
    "    score, index = torch.topk(p, 1)\n",
    "    input_category_id = index[0][0].item()\n",
    "    predicted_confidence = score[0][0].item()\n",
    "    predicted_confidence = (\"%.2f\") %(predicted_confidence)\n",
    "#     print(predicted_confidence)\n",
    "\n",
    "\n",
    "    input_prediction_id = convert_imagenet_label_to_id(label_map, key_list, val_list, input_category_id)\n",
    "    predicted_label = id_map.get(input_prediction_id).split(',')[0]\n",
    "    predicted_label = predicted_label[0].lower() + predicted_label[1:]\n",
    "    \n",
    "    print(predicted_label)\n",
    "    print(predicted_confidence)\n",
    "\n",
    "    conf_dict[image_name] = predicted_confidence\n",
    "\n",
    "    # Original image\n",
    "    fig = plt.figure()\n",
    "    plt.figure(figsize=(6.0,4.5), dpi=300)\n",
    "    plt.axis('off')\n",
    "    plt.title('{}: {}'.format(predicted_label, predicted_confidence))\n",
    "    plt.imshow(input_image)\n",
    "#     plt.imshow(modified_img[idx])\n",
    "    plt.savefig('tmp/original.jpeg', bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "    img = cv.resize(cv.imread(image_path,0),((size,size)))\n",
    "    edges = cv.Canny(img,100,200)\n",
    "    edges = edges - 255\n",
    "\n",
    "    fig = plt.figure()\n",
    "    plt.figure(figsize=(6.0,4.5), dpi=300)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(edges, cmap = 'gray')\n",
    "    plt.savefig('tmp/Edge.jpeg', bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "#     Confidence score only\n",
    "    fig = plt.figure()\n",
    "    plt.figure(figsize=(6.0,4.5), dpi=300)\n",
    "    plt.axis('off')\n",
    "    plt.title('{}% {}'.format(int(float(predicted_confidence)*100), predicted_label))\n",
    "    plt.imshow(input_image)\n",
    "    plt.savefig('{}/{}/Conf/{}_{}.jpeg'.format(output_path, task, image_name, int(float(predicted_confidence)*100)), bbox_inches='tight', dpi=300)\n",
    "    plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = 'GradCAM'\n",
    "# test_image_paths = corrects_bin1[methods.index(method)*img_num:methods.index(method)*img_num + img_num] + \\\n",
    "#                     wrongs_bin1[methods.index(method)*img_num:methods.index(method)*img_num + img_num] + \\\n",
    "#                     corrects_bin2[methods.index(method)*img_num:methods.index(method)*img_num + img_num] + \\\n",
    "#                     wrongs_bin2[methods.index(method)*img_num:methods.index(method)*img_num + img_num] + \\\n",
    "#                     corrects_bin3[methods.index(method)*img_num:methods.index(method)*img_num + img_num] + \\\n",
    "#                     wrongs_bin3[methods.index(method)*img_num:methods.index(method)*img_num + img_num]\n",
    "# print(len(test_image_paths))\n",
    "# for idx, image_path in enumerate(test_image_paths[methods.index(method)*img_num:methods.index(method)*img_num + img_num]):\n",
    "for idx, image_path in enumerate(test_image_paths):\n",
    "#     if 'n02106166_ILSVRC2012_val_00043367' not in image_path:\n",
    "#         continue\n",
    "#     print(len(test_image_paths[methods.index(method)*img_num:methods.index(method)*img_num + img_num]))\n",
    "    print(idx + 1)\n",
    "    distance_dict = dict()\n",
    "    neighbors = list()\n",
    "    categories_confidences = list()\n",
    "    confidences= list ()\n",
    "    img = Image.open(image_path)\n",
    "    \n",
    "    if NN_flag:\n",
    "        embedding = feature_extractor(transform(img).unsqueeze(0).to(device)).flatten(start_dim=1) \n",
    "#     print(test_image_paths[idx])\n",
    "    input_image = img.resize((size,size), Image.ANTIALIAS)\n",
    "\n",
    "    image_name = (image_path.split('.jpeg')[0]).split('images/')[1]\n",
    "    print(image_name)\n",
    "\n",
    "    # Get the ground truth of the input image\n",
    "    gt_label_id = image_path.split('val_')[1][9:18]\n",
    "    \n",
    "    gt_label = id_map.get(gt_label_id)\n",
    "    id = key_list.index(gt_label_id)\n",
    "    gt_label = gt_label.split(',')[0]\n",
    "\n",
    "\n",
    "    # Get the prediction for the input image\n",
    "    img = Image.open(image_path)\n",
    "    x = transform(img).unsqueeze(0).to(device)\n",
    "    out = model(x)\n",
    "    p = torch.nn.functional.softmax(out, dim=1)\n",
    "    score, index = torch.topk(p, 1)\n",
    "    input_category_id = index[0][0].item()\n",
    "    predicted_confidence = score[0][0].item()\n",
    "    predicted_confidence = (\"%.2f\") %(predicted_confidence)\n",
    "#     print(predicted_confidence)\n",
    "\n",
    "\n",
    "    input_prediction_id = convert_imagenet_label_to_id(label_map, key_list, val_list, input_category_id)\n",
    "    predicted_label = id_map.get(input_prediction_id).split(',')[0]\n",
    "    predicted_label = predicted_label[0].lower() + predicted_label[1:]\n",
    "    \n",
    "#     print(gt_label_id, input_prediction_id)\n",
    "    \n",
    "    conf_dict[image_name] = predicted_confidence\n",
    "\n",
    "    # Original image\n",
    "    fig = plt.figure()\n",
    "    plt.figure(figsize=(6.0,4.5), dpi=300)\n",
    "    plt.axis('off')\n",
    "    plt.title('{}% {}'.format(int(float(predicted_confidence)*100), predicted_label))\n",
    "    plt.imshow(input_image)\n",
    "#     plt.imshow(modified_img[idx])\n",
    "    plt.savefig('tmp/original.jpeg', bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    img = cv.resize(cv.imread(image_path,0),((size,size)))\n",
    "    edges = cv.Canny(img,100,200)\n",
    "    edges = edges - 255\n",
    "\n",
    "    fig = plt.figure()\n",
    "    plt.figure(figsize=(6.0,4.5), dpi=300)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(edges, cmap = 'gray')\n",
    "    plt.savefig('tmp/Edge.jpeg', bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "\n",
    "#     GRAD-CAM\n",
    "    saliency = grad_cam(\n",
    "        model, x, input_category_id,\n",
    "        saliency_layer='layer4',\n",
    "        resize=True\n",
    "    )\n",
    "\n",
    "    img_index = (image_path.split('.jpeg')[0]).split('images/')[1]\n",
    "    saliency_path = 'saliency_maps/GradCAM_resnet34_{}/'.format(task)\n",
    "    saliency *= 1.0/saliency.max()\n",
    "    GradCAM = saliency[0][0].cpu().detach().numpy()\n",
    "    \n",
    "#     # Save saliency maps only when the image is correctly classified\n",
    "#     if gt_label_id == input_prediction_id:\n",
    "#         np.save(os.path.join(saliency_path, \"{}.npy\".format(img_index)), GradCAM)\n",
    "\n",
    "    fig = plt.figure()\n",
    "    plt.figure(figsize=(6.0,4.5), dpi=300)\n",
    "    plt.axis('off')\n",
    "#     plt.title('Explanation')\n",
    "    plt.title('GradCAM')\n",
    "    plt.imshow(GradCAM, cmap=cMap, vmin=0, vmax=1)\n",
    "    plt.colorbar(orientation='vertical')\n",
    "    plt.savefig('tmp/heatmap.jpeg', bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    # Get overlay version\n",
    "    myCmd = 'composite -blend 10 Edge.jpeg -gravity SouthWest tmp/heatmap.jpeg tmp/overlay.jpeg'\n",
    "    os.system(myCmd)\n",
    "\n",
    "#     img_path = '{}/{}/GradCAM/{}_{}.jpeg'.format(output_path, task, image_name,int(float(predicted_confidence)*100))\n",
    "#     myCmd = 'montage original.jpeg overlay.jpeg -tile 2x1 -geometry +0+0 ' + img_path\n",
    "#     print(myCmd)\n",
    "#     os.system(myCmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = 'EP'\n",
    "#EP \n",
    "# test_image_paths = corrects_bin1[methods.index(method)*img_num:methods.index(method)*img_num + img_num] + \\\n",
    "#                     wrongs_bin1[methods.index(method)*img_num:methods.index(method)*img_num + img_num] + \\\n",
    "#                     corrects_bin2[methods.index(method)*img_num:methods.index(method)*img_num + img_num] + \\\n",
    "#                     wrongs_bin2[methods.index(method)*img_num:methods.index(method)*img_num + img_num] + \\\n",
    "#                     corrects_bin3[methods.index(method)*img_num:methods.index(method)*img_num + img_num] + \\\n",
    "#                     wrongs_bin3[methods.index(method)*img_num:methods.index(method)*img_num + img_num]\n",
    "# for idx, image_path in enumerate(test_image_paths[methods.index(method)*img_num:methods.index(method)*img_num + img_num]):\n",
    "for idx, image_path in enumerate(test_image_paths):\n",
    "#     if 'n02106166_ILSVRC2012_val_00043367' not in image_path:\n",
    "#         continue\n",
    "    print(idx + 1)\n",
    "#     if idx == 5:\n",
    "#         break\n",
    "    distance_dict = dict()\n",
    "    neighbors = list()\n",
    "    categories_confidences = list()\n",
    "    confidences= list ()\n",
    "    img = Image.open(image_path)\n",
    "    \n",
    "    if NN_flag:\n",
    "        embedding = feature_extractor(transform(img).unsqueeze(0).to(device)).flatten(start_dim=1) \n",
    "\n",
    "    input_image = img.resize((size,size), Image.ANTIALIAS)\n",
    "\n",
    "    image_name = (image_path.split('.jpeg')[0]).split('images/')[1]\n",
    "    print(image_name)\n",
    "\n",
    "    # Get the ground truth of the input image\n",
    "    gt_label_id = image_path.split('val_')[1][9:18]\n",
    "\n",
    "    \n",
    "    gt_label = id_map.get(gt_label_id)\n",
    "    id = key_list.index(gt_label_id)\n",
    "    gt_label = gt_label.split(',')[0]\n",
    "\n",
    "\n",
    "    # Get the prediction for the input image\n",
    "    img = Image.open(image_path)\n",
    "    x = transform(img).unsqueeze(0).to(device)\n",
    "    out = model(x)\n",
    "    p = torch.nn.functional.softmax(out, dim=1)\n",
    "    score, index = torch.topk(p, 1)\n",
    "    input_category_id = index[0][0].item()\n",
    "    predicted_confidence = score[0][0].item()\n",
    "    predicted_confidence = (\"%.2f\") %(predicted_confidence)\n",
    "#     print(predicted_confidence)\n",
    "\n",
    "\n",
    "    input_prediction_id = convert_imagenet_label_to_id(label_map, key_list, val_list, input_category_id)\n",
    "    predicted_label = id_map.get(input_prediction_id).split(',')[0]\n",
    "    predicted_label = predicted_label[0].lower() + predicted_label[1:]\n",
    "    \n",
    "    if input_prediction_id != gt_label_id:\n",
    "        continue\n",
    "\n",
    "    conf_dict[image_name] = predicted_confidence\n",
    "\n",
    "    # Original image\n",
    "    fig = plt.figure()\n",
    "    plt.figure(figsize=(6.0,4.5), dpi=300)\n",
    "    plt.axis('off')\n",
    "    plt.title('{}% {}'.format(int(float(predicted_confidence)*100), predicted_label))\n",
    "    plt.imshow(input_image)\n",
    "#     plt.imshow(modified_img[idx])\n",
    "    plt.savefig('tmp/original.jpeg', bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    img = cv.resize(cv.imread(image_path,0),((size,size)))\n",
    "    edges = cv.Canny(img,100,200)\n",
    "    edges = edges - 255\n",
    "\n",
    "    fig = plt.figure()\n",
    "    plt.figure(figsize=(6.0,4.5), dpi=300)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(edges, cmap = 'gray')\n",
    "    plt.savefig('tmp/Edge.jpeg', bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "\n",
    "    # Extremal Perturbation\n",
    "    saliency,m0,m1,m2,m3 = get_EP_saliency_maps(model, image_path)\n",
    "    ep_saliency_map = tensor2img(saliency)\n",
    "    ep_saliency_map *= 1.0/ep_saliency_map.max()\n",
    "\n",
    "#     ep_saliency_map = np.mean(ep_saliency_map, axis=-1)\n",
    "\n",
    "    fig = plt.figure()\n",
    "    plt.figure(figsize=(6.0,4.5), dpi=300)\n",
    "    plt.axis('off')\n",
    "#     plt.title('Explanation')\n",
    "    plt.title('EP')\n",
    "    plt.imshow(ep_saliency_map, cmap=cMap, vmin=0, vmax=1)\n",
    "    plt.colorbar(orientation='vertical')\n",
    "    plt.savefig('tmp/heatmap.jpeg', bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    # Get overlay version\n",
    "    myCmd = 'composite -blend 10 tmp/Edge.jpeg -gravity SouthWest tmp/heatmap.jpeg tmp/overlay.jpeg'\n",
    "    os.system(myCmd)\n",
    "\n",
    "#     img_path = '{}/{}/EP/{}_{}.jpeg'.format(output_path, task, image_name, int(float(predicted_confidence)*100))\n",
    "#     myCmd = 'montage original.jpeg overlay.jpeg -tile 2x1 -geometry +0+0 ' + img_path\n",
    "#     os.system(myCmd)\n",
    "\n",
    "print(\"Done!\")\n",
    "#----------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method = 'SHAP'\n",
    "# # SHAP\n",
    "# test_image_paths = corrects_bin1[methods.index(method)*img_num:methods.index(method)*img_num + img_num] + \\\n",
    "#                     wrongs_bin1[methods.index(method)*img_num:methods.index(method)*img_num + img_num] + \\\n",
    "#                     corrects_bin2[methods.index(method)*img_num:methods.index(method)*img_num + img_num] + \\\n",
    "#                     wrongs_bin2[methods.index(method)*img_num:methods.index(method)*img_num + img_num] + \\\n",
    "#                     corrects_bin3[methods.index(method)*img_num:methods.index(method)*img_num + img_num] + \\\n",
    "#                     wrongs_bin3[methods.index(method)*img_num:methods.index(method)*img_num + img_num]\n",
    "\n",
    "# # for idx, image_path in enumerate(test_image_paths[methods.index(method)*img_num:methods.index(method)*img_num + img_num]):\n",
    "# for idx, image_path in enumerate(test_image_paths):\n",
    "#     print(idx + 1)\n",
    "#     distance_dict = dict()\n",
    "#     neighbors = list()\n",
    "#     categories_confidences = list()\n",
    "#     confidences= list ()\n",
    "#     img = Image.open(image_path)\n",
    "    \n",
    "#     if NN_flag:\n",
    "#         embedding = feature_extractor(transform(img).unsqueeze(0).to(device)).flatten(start_dim=1) \n",
    "# #     print(test_image_paths[idx])\n",
    "#     input_image = img.resize((size,size), Image.ANTIALIAS)\n",
    "#     if adv_flag:\n",
    "#         image_name = (image_path.split('.jpeg')[0]).split('images/')[1]\n",
    "#     else:\n",
    "#         image_name = (image_path.split('.jpeg')[0]).split('images/')[1]\n",
    "#     print(image_name)\n",
    "\n",
    "#     # Get the ground truth of the input image\n",
    "#     if adv_flag:\n",
    "#         gt_label_id = image_path.split('val_')[1][9:18]\n",
    "#     else:\n",
    "#         gt_label_id = image_path[-14:-5]\n",
    "\n",
    "    \n",
    "#     gt_label = id_map.get(gt_label_id)\n",
    "#     id = key_list.index(gt_label_id)\n",
    "#     gt_label = gt_label.split(',')[0]\n",
    "\n",
    "\n",
    "#     # Get the prediction for the input image\n",
    "#     img = Image.open(image_path)\n",
    "#     x = transform(img).unsqueeze(0).to(device)\n",
    "#     out = model(x)\n",
    "#     p = torch.nn.functional.softmax(out, dim=1)\n",
    "#     score, index = torch.topk(p, 1)\n",
    "#     input_category_id = index[0][0].item()\n",
    "#     predicted_confidence = score[0][0].item()\n",
    "#     predicted_confidence = (\"%.2f\") %(predicted_confidence)\n",
    "# #     print(predicted_confidence)\n",
    "\n",
    "\n",
    "#     input_prediction_id = convert_imagenet_label_to_id(label_map, key_list, val_list, input_category_id)\n",
    "#     predicted_label = id_map.get(input_prediction_id).split(',')[0]\n",
    "#     predicted_label = predicted_label[0].lower() + predicted_label[1:]\n",
    "\n",
    "#     conf_dict[image_name] = predicted_confidence\n",
    "\n",
    "#     # Original image\n",
    "#     fig = plt.figure()\n",
    "#     plt.figure(figsize=(6.0,4.5), dpi=300)\n",
    "#     plt.axis('off')\n",
    "#     plt.title('{}: {}'.format(predicted_label, predicted_confidence))\n",
    "#     plt.imshow(input_image)\n",
    "# #     plt.imshow(modified_img[idx])\n",
    "#     plt.savefig('original.jpeg', bbox_inches='tight')\n",
    "#     plt.close()\n",
    "\n",
    "#     img = cv.resize(cv.imread(image_path,0),((size,size)))\n",
    "#     edges = cv.Canny(img,100,200)\n",
    "#     edges = edges - 255\n",
    "\n",
    "#     fig = plt.figure()\n",
    "#     plt.figure(figsize=(6.0,4.5), dpi=300)\n",
    "#     plt.axis('off')\n",
    "#     plt.imshow(edges, cmap = 'gray')\n",
    "#     plt.savefig('Edge.jpeg', bbox_inches='tight')\n",
    "#     plt.close()\n",
    "    \n",
    "\n",
    "#     test_images = Image.open(image_path)\n",
    "#     test_images = [transform(test_images).unsqueeze(0)]\n",
    "#     test_images = [i.to(device) for i in test_images]\n",
    "\n",
    "#     test_images_t = torch.cat(test_images)\n",
    "#     shap_values = e.shap_values(test_images_t, ranked_outputs=1, output_rank_order='max') # 2xranked_outputsx1x3x224x224\n",
    "\n",
    "#     square_img = np.sum(shap_values[0][0][0], axis=0)\n",
    "#     min_val = square_img.min()\n",
    "#     max_val = square_img.max()\n",
    "#     if (-1*min_val) > max_val:\n",
    "#         max_val = -1.0*min_val\n",
    "\n",
    "#     square_img *= 1.0/max_val\n",
    "#     fig = plt.figure()\n",
    "#     plt.figure(figsize=(6.0,4.5), dpi=300)\n",
    "#     plt.axis('off')\n",
    "#     plt.title('Explanation')\n",
    "#     plt.imshow(square_img, cmap=cMap, vmin=-1, vmax=1)\n",
    "\n",
    "#     plt.colorbar(orientation='vertical')\n",
    "#     plt.savefig('heatmap.jpeg', bbox_inches='tight')\n",
    "#     plt.close()\n",
    "\n",
    "#     # Get overlay version\n",
    "#     myCmd = 'composite -blend 10 Edge.jpeg -gravity SouthWest heatmap.jpeg overlay.jpeg'\n",
    "#     os.system(myCmd)\n",
    "\n",
    "#     img_path = 'Pilot_study2/{}/SHAP/{}.jpeg'.format(task, image_name)\n",
    "#     myCmd = 'montage original.jpeg overlay.jpeg -tile 2x1 -geometry +0+0 ' + img_path\n",
    "#     os.system(myCmd)\n",
    "\n",
    "#     img_index = (image_path.split('.jpeg')[0]).split('images/')[1]\n",
    "#     saliency_t = (shap_values[0][0])[0, 0, :]\n",
    "#     saliency_path = 'saliency_maps/SHAP/'\n",
    "#     if not (os.path.exists(saliency_path)):\n",
    "#         os.mkdir(saliency_path)\n",
    "#     np.save(os.path.join(saliency_path, \"{}_.npy\".format(img_index)), saliency_t)\n",
    "\n",
    "# #----------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = 'NNs'\n",
    "# NNs\n",
    "# test_image_paths = corrects_bin1[methods.index(method)*img_num:methods.index(method)*img_num + img_num] + \\\n",
    "#                     wrongs_bin1[methods.index(method)*img_num:methods.index(method)*img_num + img_num] + \\\n",
    "#                     corrects_bin2[methods.index(method)*img_num:methods.index(method)*img_num + img_num] + \\\n",
    "#                     wrongs_bin2[methods.index(method)*img_num:methods.index(method)*img_num + img_num] + \\\n",
    "#                     corrects_bin3[methods.index(method)*img_num:methods.index(method)*img_num + img_num] + \\\n",
    "#                     wrongs_bin3[methods.index(method)*img_num:methods.index(method)*img_num + img_num]\n",
    "\n",
    "# for idx, image_path in enumerate(test_image_paths[methods.index(method)*img_num:methods.index(method)*img_num + img_num]):\n",
    "for idx, image_path in enumerate(test_image_paths):\n",
    "#     if 'n02106166_ILSVRC2012_val_00043367' not in image_path:\n",
    "#         continue\n",
    "#     if idx == 2:\n",
    "#         break\n",
    "    print(idx + 1)\n",
    "#     if \"32048\" not in image_path:\n",
    "#         continue\n",
    "    distance_dict = dict()\n",
    "    neighbors = list()\n",
    "    categories_confidences = list()\n",
    "    confidences= list ()\n",
    "    img = Image.open(image_path)\n",
    "    \n",
    "    if NN_flag:\n",
    "        embedding = feature_extractor(transform(img).unsqueeze(0).to(device)).flatten(start_dim=1) \n",
    "#     print(test_image_paths[idx])\n",
    "    input_image = img.resize((size,size), Image.ANTIALIAS)\n",
    "    \n",
    "    image_name = (image_path.split('.jpeg')[0]).split('images/')[1]\n",
    "    print(image_name)\n",
    "\n",
    "    # Get the ground truth of the input image\n",
    "    gt_label_id = image_path.split('val_')[1][9:18]\n",
    "\n",
    "    gt_label = id_map.get(gt_label_id)\n",
    "    id = key_list.index(gt_label_id)\n",
    "    gt_label = gt_label.split(',')[0]\n",
    "\n",
    "\n",
    "    # Get the prediction for the input image\n",
    "    img = Image.open(image_path)\n",
    "    x = transform(img).unsqueeze(0).to(device)\n",
    "    out = model(x)\n",
    "    p = torch.nn.functional.softmax(out, dim=1)\n",
    "    score, index = torch.topk(p, 1)\n",
    "    input_category_id = index[0][0].item()\n",
    "    predicted_confidence = score[0][0].item()\n",
    "    predicted_confidence = (\"%.2f\") %(predicted_confidence)\n",
    "#     print(predicted_confidence)\n",
    "\n",
    "\n",
    "#     if os.path.exists('{}/{}/NNs/{}_{}.jpeg'.format(output_path, task, image_name, int(float(predicted_confidence)*100))) == True:\n",
    "#         continue\n",
    "\n",
    "\n",
    "    input_prediction_id = convert_imagenet_label_to_id(label_map, key_list, val_list, input_category_id)\n",
    "    predicted_label = id_map.get(input_prediction_id).split(',')[0]\n",
    "    predicted_label = predicted_label[0].lower() + predicted_label[1:]\n",
    "\n",
    "    conf_dict[image_name] = predicted_confidence\n",
    "\n",
    "    # Original image\n",
    "    fig = plt.figure()\n",
    "    plt.figure(figsize=(6.0,4.5), dpi=300)\n",
    "    plt.axis('off')\n",
    "    plt.title('{}% {}'.format(int(float(predicted_confidence)*100), predicted_label))\n",
    "    plt.imshow(input_image)\n",
    "#     plt.imshow(modified_img[idx])\n",
    "    plt.savefig('tmp/original.jpeg', bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    img = cv.resize(cv.imread(image_path,0),((size,size)))\n",
    "    edges = cv.Canny(img,100,200)\n",
    "    edges = edges - 255\n",
    "\n",
    "    fig = plt.figure()\n",
    "    plt.figure(figsize=(6.0,4.5), dpi=300)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(edges, cmap = 'gray')\n",
    "    plt.savefig('tmp/Edge.jpeg', bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    if NN_flag:\n",
    "        from utils import *\n",
    "        ## Nearest Neighbors\n",
    "\n",
    "        predicted_set_path = os.path.join(imagenet_train_path, input_prediction_id)\n",
    "        predicted_set_img_paths = glob.glob(predicted_set_path + '/*.*')\n",
    "        predicted_set_color_images= list()\n",
    "\n",
    "        embedding = embedding.detach()\n",
    "        embedding.to(device)\n",
    "        # Build search space for nearest neighbors\n",
    "        for i, path in enumerate(predicted_set_img_paths):\n",
    "            img = Image.open(path)\n",
    "            if img.mode != 'RGB':\n",
    "                img.close()\n",
    "                del img\n",
    "                continue\n",
    "\n",
    "            x = transform(img).unsqueeze(0).to(device)\n",
    "            out = model(x)\n",
    "            p = torch.nn.functional.softmax(out, dim=1)\n",
    "            del out\n",
    "            score, index = torch.topk(p, 1)\n",
    "            del p\n",
    "            category_id = index[0][0].item()\n",
    "            del score, index\n",
    "            \n",
    "#             This is to avoid the confusion from crane 134 and crane 517 and to make NNs work :)\n",
    "#             Because in Imagenet, annotators mislabeled 134 and 517\n",
    "\n",
    "            if input_category_id != 134 and input_category_id != 517 and category_id != 134 and category_id != 517:\n",
    "                if input_category_id != category_id:\n",
    "                    continue\n",
    "\n",
    "            f = feature_extractor(x)\n",
    "            feature_vector = f.flatten(start_dim=1).to(device)\n",
    "            feature_vector = feature_vector.detach()\n",
    "\n",
    "            del f\n",
    "#             del x\n",
    "#             Tracer()()\n",
    "            distance_dict[path] = torch.dist(embedding, feature_vector)\n",
    "\n",
    "            del feature_vector \n",
    "            torch.cuda.empty_cache()\n",
    "            img.close()\n",
    "            del img\n",
    "            predicted_set_color_images.append(path)\n",
    "\n",
    "        # Get K most similar images\n",
    "        res = dict(sorted(distance_dict.items(), key = itemgetter(1))[:K]) \n",
    "        print(\"Before...\")\n",
    "        print(res)\n",
    "#         Tracer()()\n",
    "        while distance_dict[list(res.keys())[0]] < 100:\n",
    "            del distance_dict[list(res.keys())[0]]\n",
    "            res = dict(sorted(distance_dict.items(), key = itemgetter(1))[:K]) \n",
    "        print(\"After...\")\n",
    "        print(res)\n",
    "#         del distance_dict\n",
    "        del embedding\n",
    "\n",
    "        similar_images = list(res.keys())\n",
    "#         print(similar_images)\n",
    "#         print([distance_dict[x] for x in similar_images])\n",
    "\n",
    "        for similar_image in similar_images:\n",
    "            img = Image.open(similar_image)\n",
    "            neighbors.append(img.resize((size,size), Image.ANTIALIAS))\n",
    "            x = transform(img).unsqueeze(0).to(device)\n",
    "            out = model(x)\n",
    "            p = torch.nn.functional.softmax(out, dim=1)\n",
    "            score, index = torch.topk(p, 1) # Get 1 most probable classes\n",
    "            category_id = index[0][0].item()\n",
    "            confidence = score[0][0].item()\n",
    "\n",
    "            label = label_map.get(category_id).split(',')[0].replace(\"\\\"\", \"\")\n",
    "            label = label[0].lower() + label[1:]\n",
    "            print(label + \": %.2f\" %(confidence))\n",
    "\n",
    "            categories_confidences.append((label + \": %.2f\" %(confidence)))\n",
    "            confidences.append(confidence)\n",
    "            img.close()\n",
    "            \n",
    "        img_path = '{}/{}/NNs/{}_{}.jpeg'.format(output_path, task, image_name, int(float(predicted_confidence)*100))\n",
    "        print(img_path)\n",
    "        font = {'color': 'red',\n",
    "               'weight': 'bold',\n",
    "               }\n",
    "        for index, neighbor in enumerate(neighbors):\n",
    "            fig = plt.figure()\n",
    "            plt.figure(figsize=(6.0,4.5), dpi=300)\n",
    "            plt.axis('off')\n",
    "#             plt.title('{}'.format(categories_confidences[index]))\n",
    "            if index == 1: # Make title for the middle image (2nd image) to annotate the 3 NNs\n",
    "#                 plt.title('Three examples of {}'.format(predicted_label), fontdict=font)\n",
    "                plt.title('3-NNs'.format(predicted_label))\n",
    "            else:\n",
    "                plt.title(' ')\n",
    "            plt.imshow(neighbor)\n",
    "            plt.savefig('tmp/{}.jpeg'.format(index), bbox_inches='tight')\n",
    "            plt.close()\n",
    "            \n",
    "        myCmd = 'montage tmp/[0-2].jpeg -tile 3x1 -geometry +0+0 ' + 'tmp/aggregate.jpeg'\n",
    "        os.system(myCmd)\n",
    "\n",
    "#         myCmd = 'montage original.jpeg ' + 'aggregate.jpeg' + ' -tile 2x -geometry +120+0 ' + img_path\n",
    "#         os.system(myCmd)\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = 'PoolNet'\n",
    "from shutil import copyfile, rmtree\n",
    "def rm_and_mkdir(path):\n",
    "    if os.path.isdir(path) == True:\n",
    "        rmtree(path)\n",
    "    os.mkdir(path)\n",
    "\n",
    "# Prepare dataset\n",
    "rm_and_mkdir('/home/dexter/Downloads/run-0/run-0-sal-p/')\n",
    "rm_and_mkdir('/home/dexter/Downloads/PoolNet-master/data/PASCALS/Imgs/')\n",
    "if os.path.isdir('/home/dexter/Downloads/PoolNet-master/data/PASCALS/test.lst'):\n",
    "    os.remove('/home/dexter/Downloads/PoolNet-master/data/PASCALS/test.lst')\n",
    "\n",
    "# test_image_paths = corrects_bin1[methods.index(method)*img_num:methods.index(method)*img_num + img_num] + \\\n",
    "#                     wrongs_bin1[methods.index(method)*img_num:methods.index(method)*img_num + img_num] + \\\n",
    "#                     corrects_bin2[methods.index(method)*img_num:methods.index(method)*img_num + img_num] + \\\n",
    "#                     wrongs_bin2[methods.index(method)*img_num:methods.index(method)*img_num + img_num] + \\\n",
    "#                     corrects_bin3[methods.index(method)*img_num:methods.index(method)*img_num + img_num] + \\\n",
    "#                     wrongs_bin3[methods.index(method)*img_num:methods.index(method)*img_num + img_num]\n",
    "\n",
    "# src_paths = test_image_paths[methods.index(method)*img_num:methods.index(method)*img_num + img_num]\n",
    "src_paths = test_image_paths\n",
    "for src_path in src_paths:\n",
    "    dst_path = '/home/dexter/Downloads/PoolNet-master/data/PASCALS/Imgs/' + src_path.split('images/')[1]\n",
    "    copyfile(src_path, dst_path)\n",
    "\n",
    "cmd = 'ls /home/dexter/Downloads/PoolNet-master/data/PASCALS/Imgs/ > /home/dexter/Downloads/PoolNet-master/data/PASCALS/test.lst'\n",
    "os.system(cmd)\n",
    "cmd = 'python /home/dexter/Downloads/PoolNet-master/main.py --mode=\\'test\\' --model=\\'/home/dexter/Downloads/run-0/run-0/models/final.pth\\' --test_fold=\\'/home/dexter/Downloads/run-0/run-0-sal-p/\\' --sal_mode=\\'p\\''\n",
    "os.system(cmd)\n",
    "\n",
    "\n",
    "npy_file_paths = glob.glob('/home/dexter/Downloads/run-0/run-0-sal-p/*.*')\n",
    "print(len(npy_file_paths))\n",
    "for idx, npy_file_path in enumerate(npy_file_paths): \n",
    "#     if 'n02106166_ILSVRC2012_val_00043367' not in npy_file_path:\n",
    "#         continue\n",
    "    print(idx)\n",
    "    test_path = '/home/dexter/Downloads/PoolNet-master/data/PASCALS/Imgs' + npy_file_path.split('.npy')[0].split('sal-p')[1]\n",
    "    image_name = npy_file_path.split('.npy')[0].split('sal-p')[1]\n",
    "    img = Image.open(test_path)\n",
    "    input_image = img.resize((size,size), Image.ANTIALIAS)\n",
    "    \n",
    "    # Retrieve confidence and label\n",
    "    img = Image.open(test_path)\n",
    "    x = transform(img).unsqueeze(0).to(device)\n",
    "    out = model(x)\n",
    "    p = torch.nn.functional.softmax(out, dim=1)\n",
    "    score, index = torch.topk(p, 1)\n",
    "    input_category_id = index[0][0].item()\n",
    "    predicted_confidence = score[0][0].item()\n",
    "    predicted_confidence = (\"%.2f\") %(predicted_confidence)\n",
    "    \n",
    "    if os.path.exists('{}/{}/PoolNet/{}_{}.jpeg'.format(output_path, task, image_name, int(float(predicted_confidence)*100))) == True:\n",
    "        print(\"Continue\")\n",
    "        continue\n",
    "    \n",
    "    # For overlay heatmap\n",
    "    img = cv.resize(cv.imread(test_path,0),((size,size)))\n",
    "    edges = cv.Canny(img,100,200)\n",
    "    edges = edges - 255\n",
    "\n",
    "    fig = plt.figure()\n",
    "    plt.figure(figsize=(6.0,4.5), dpi=300)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(edges, cmap = 'gray')\n",
    "    plt.savefig('tmp/Edge.jpeg', bbox_inches='tight')\n",
    "    plt.close()\n",
    "    ##############################\n",
    "\n",
    "    \n",
    "    input_prediction_id = convert_imagenet_label_to_id(label_map, key_list, val_list, input_category_id)\n",
    "    predicted_label = id_map.get(input_prediction_id).split(',')[0]\n",
    "    predicted_label = predicted_label[0].lower() + predicted_label[1:]\n",
    "    \n",
    "    # Plot input image\n",
    "    fig = plt.figure()\n",
    "    plt.figure(figsize=(6.0,4.5), dpi=300)\n",
    "    plt.axis('off')\n",
    "    plt.title('{}% {}'.format(int(float(predicted_confidence)*100), predicted_label))\n",
    "    plt.imshow(input_image)\n",
    "    plt.savefig('tmp/original.jpeg', bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # Plot the explanation\n",
    "    npy_file = np.load(npy_file_path)\n",
    "    fig = plt.figure()\n",
    "    plt.figure(figsize=(6.0,4.5), dpi=300)\n",
    "    plt.axis('off')\n",
    "#     plt.title('Explanation')\n",
    "    plt.title('SOD')\n",
    "    plt.imshow(npy_file, cmap=cMap, vmin=0, vmax=1)\n",
    "    plt.colorbar(orientation='vertical')\n",
    "    plt.savefig('tmp/heatmap.jpeg', bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # Get overlay version\n",
    "    myCmd = 'composite -blend 10 tmp/Edge.jpeg -gravity SouthWest tmp/heatmap.jpeg tmp/overlay.jpeg'\n",
    "    os.system(myCmd)\n",
    "\n",
    "#     image_name = image_name.split('.jpeg')[0]\n",
    "#     img_path = '{}/{}/PoolNet/{}_{}.jpeg'.format(output_path, task, image_name, int(float(predicted_confidence)*100))\n",
    "#     myCmd = 'montage original.jpeg overlay.jpeg -tile 2x1 -geometry +0+0 ' + img_path\n",
    "#     os.system(myCmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n",
      "300\n",
      "Accuracy at T = 0.55 is 0.18666666666666668\n",
      "Best accuracy is 0.18666666666666668 at T = 0.55\n"
     ]
    }
   ],
   "source": [
    "method = 'AIonly'\n",
    "# test_image_paths = test_image_paths[methods.index(method)*img_num:methods.index(method)*img_num + img_num]\n",
    "# test_image_paths = corrects_bin1[methods.index(method)*img_num:methods.index(method)*img_num + img_num] + \\\n",
    "#                     wrongs_bin1[methods.index(method)*img_num:methods.index(method)*img_num + img_num] + \\\n",
    "#                     corrects_bin2[methods.index(method)*img_num:methods.index(method)*img_num + img_num] + \\\n",
    "#                     wrongs_bin2[methods.index(method)*img_num:methods.index(method)*img_num + img_num] + \\\n",
    "#                     corrects_bin3[methods.index(method)*img_num:methods.index(method)*img_num + img_num] + \\\n",
    "#                     wrongs_bin3[methods.index(method)*img_num:methods.index(method)*img_num + img_num]\n",
    "\n",
    "conf_dict = dict()\n",
    "print(len(test_image_paths))\n",
    "for test_image_path in test_image_paths:\n",
    "    img = Image.open(test_image_path)\n",
    "    gt_label_id = test_image_path[-9:]\n",
    "    image_name = (test_image_path.split('.JPEG')[0]).split('images/')[1]\n",
    "    x = transform(img).unsqueeze(0).to(device)\n",
    "    out = model(x)\n",
    "    \n",
    "    p = torch.nn.functional.softmax(out, dim=1)\n",
    "    score, index = torch.topk(p, 1)\n",
    "    input_category_id = index[0][0].item()\n",
    "    input_prediction_id = convert_imagenet_label_to_id(label_map, key_list, val_list, input_category_id)\n",
    "\n",
    "    predicted_label = id_map.get(input_prediction_id).split(',')[0]\n",
    "    predicted_confidence = score[0][0].item()\n",
    "    predicted_confidence = (\"%.2f\") %(predicted_confidence)\n",
    "#     print([image_name, predicted_confidence])\n",
    "    conf_dict[image_name] = predicted_confidence\n",
    "print(len(conf_dict))\n",
    "\n",
    "correct_images = glob.glob('{}/{}/correct_bin2_images/*.*'.format(dataset_path, task))\n",
    "wrong_images = glob.glob('{}/{}/wrong_bin2_images/*.*'.format(dataset_path, task))\n",
    "\n",
    "# correct_images = list()\n",
    "\n",
    "correct_indices = list()\n",
    "wrong_indices = list()\n",
    "\n",
    "gt_dict = dict()\n",
    "\n",
    "for correct_image in correct_images:\n",
    "    image_idx = (correct_image.split('.JPEG')[0]).split('images/')[1]\n",
    "    gt_dict[image_idx] = True\n",
    "    correct_indices.append(image_idx)\n",
    "\n",
    "for wrong_image in wrong_images:\n",
    "    image_idx = (wrong_image.split('.JPEG')[0]).split('images/')[1]\n",
    "    gt_dict[image_idx] = False\n",
    "    wrong_indices.append(image_idx)\n",
    "    \n",
    "\n",
    "accuracy_T = list()\n",
    "# print(gt_dict)\n",
    "\n",
    "# T_list = list(np.arange(0.0,1.0,0.05))\n",
    "T_list = [0.55]\n",
    "for T in T_list:\n",
    "#     print(T)\n",
    "    conf_dict_T = dict()\n",
    "    for key in conf_dict.keys():\n",
    "        conf = float(conf_dict[key])\n",
    "        if conf > T:\n",
    "            conf_dict_T[key] = True\n",
    "        else:\n",
    "            conf_dict_T[key] = False\n",
    "#     print(conf_dict_T)\n",
    "    shared_items = {k: gt_dict[k] for k in gt_dict if k in conf_dict_T and gt_dict[k] == conf_dict_T[k]}\n",
    "    print(\"Accuracy at T = {} is {}\".format(T, len(shared_items)/len(test_image_paths)))\n",
    "    accuracy_T.append(len(shared_items))\n",
    "\n",
    "\n",
    "print(\"Best accuracy is {} at T = {}\".format(max(accuracy_T)/len(test_image_paths), T_list[accuracy_T.index(max(accuracy_T))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(task)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "NearestNeighbors_v2_vs_CAM_modified.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
