{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze ImageNet expert experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  \n",
    "import glob\n",
    "import os\n",
    "\n",
    "tasks = ['Natural', 'Dog', 'Adversarial_Nat', 'Adversarial_Dog']\n",
    "dataset_path = '/home/dexter/Downloads/Human_experiments/Dataset'\n",
    "ground_truth = dict()\n",
    "\n",
    "# Build the ground truth dictionary \n",
    "\n",
    "CORRECT_BIN1_IMAGES = 'correct_bin1_images/'\n",
    "CORRECT_BIN2_IMAGES = 'correct_bin2_images/'\n",
    "CORRECT_BIN3_IMAGES = 'correct_bin3_images/'\n",
    "WRONG_BIN1_IMAGES = 'wrong_bin1_images/'\n",
    "WRONG_BIN2_IMAGES = 'wrong_bin2_images/'\n",
    "WRONG_BIN3_IMAGES = 'wrong_bin3_images/'\n",
    "bin_images = [CORRECT_BIN1_IMAGES, CORRECT_BIN2_IMAGES, CORRECT_BIN3_IMAGES, WRONG_BIN1_IMAGES, WRONG_BIN2_IMAGES, WRONG_BIN3_IMAGES]\n",
    "easy_bins = [WRONG_BIN1_IMAGES, CORRECT_BIN3_IMAGES]\n",
    "hard_bins = [CORRECT_BIN1_IMAGES, WRONG_BIN3_IMAGES]\n",
    "norm_bins = [CORRECT_BIN2_IMAGES, WRONG_BIN2_IMAGES]\n",
    "\n",
    "\n",
    "for task in tasks:\n",
    "    ground_truth[task] = dict()\n",
    "    task_dataset_path = os.path.join(dataset_path,task)\n",
    "    for CORRECT_BIN_IMAGES in [CORRECT_BIN1_IMAGES, CORRECT_BIN2_IMAGES, CORRECT_BIN3_IMAGES]:\n",
    "        bin_gt_dict = dict()\n",
    "        correct_bin_images_path = os.path.join(task_dataset_path, CORRECT_BIN_IMAGES)\n",
    "        correct_bin_images = [path.split(CORRECT_BIN_IMAGES)[1] for path in glob.glob(correct_bin_images_path + '/*.*')]\n",
    "        for correct_bin_image in correct_bin_images:\n",
    "            bin_gt_dict[correct_bin_image] = 'Yes'\n",
    "        \n",
    "        ground_truth[task][CORRECT_BIN_IMAGES] = bin_gt_dict\n",
    "        \n",
    "    for WRONG_BIN_IMAGES in [WRONG_BIN1_IMAGES, WRONG_BIN2_IMAGES, WRONG_BIN3_IMAGES]:\n",
    "        bin_gt_dict = dict()\n",
    "        wrong_bin_images_path = os.path.join(task_dataset_path, WRONG_BIN_IMAGES)\n",
    "        wrong_bin_images = [path.split(WRONG_BIN_IMAGES)[1] for path in glob.glob(wrong_bin_images_path + '/*.*')]\n",
    "        for wrong_bin_image in wrong_bin_images:\n",
    "            bin_gt_dict[wrong_bin_image] = 'No'\n",
    "        \n",
    "        ground_truth[task][WRONG_BIN_IMAGES] = bin_gt_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  \n",
    "import operator\n",
    "import os\n",
    "import math\n",
    "import statistics\n",
    "from IPython.core.debugger import Tracer\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "%matplotlib inline\n",
    "\n",
    "# Check if a number is float?\n",
    "def is_float(s):\n",
    "    try:\n",
    "        float(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "    \n",
    "# Task aliases assigned by Gorilla\n",
    "exp_hierarchy = ['randomiser-tjl7', {'GradCAM': 'counterbalance-ao9d', 'NNs': 'counterbalance-ao9d'}]\n",
    "\n",
    "methods = ['GradCAM', 'NNs']\n",
    "dataset_path = '/home/dexter/Downloads/Human_experiments/Dataset'\n",
    "\n",
    "# Loop all csv file, each csv file is an explanation method\n",
    "files = []\n",
    "\n",
    "exp = 'Natural'\n",
    "for file in glob.glob('Expert_Data/{}/*.*'.format(exp)):\n",
    "    if '.csv' in file:\n",
    "        files.append(file)\n",
    "\n",
    "if exp == 'Natural':\n",
    "    tasks = ['Natural', 'Adversarial_Nat']\n",
    "    threshold = 8\n",
    "else:\n",
    "    tasks = ['Dog', 'Adversarial_Dog']\n",
    "    threshold = 8\n",
    "    \n",
    "csv_file = open('csv_files/{}_bad_users.csv'.format(tasks[0]), 'w')\n",
    "final_result = dict()\n",
    "trial_cnt = dict()\n",
    "\n",
    "# Initialize the dictionary for users' responses (answer)\n",
    "for task in tasks:\n",
    "    final_result[task] = dict()\n",
    "    trial_cnt[task] = dict()\n",
    "    for method in methods:\n",
    "        final_result[task][method] = dict()\n",
    "        trial_cnt[task][method] = dict()\n",
    "        for bin_image in bin_images:\n",
    "            final_result[task][method][bin_image] = 0\n",
    "            trial_cnt[task][method][bin_image] = 0\n",
    "\n",
    "# Correct answer dictionary in validation \n",
    "val_correct_dict = dict()\n",
    "# Incorrect answer dictionary in validation \n",
    "val_incorrect_trials_dict = dict()\n",
    "\n",
    "test_correct_trials_dict = dict()\n",
    "test_incorrect_trials_dict = dict()\n",
    "\n",
    "# Numbers of users for methods\n",
    "user_cnt_dict = dict()\n",
    "\n",
    "# Reaction time dictionary \n",
    "users_avg_reaction_time_dict = dict()\n",
    "users_stdev_reaction_time_dict = dict()\n",
    "below_stdev_reaction_time_dict = dict()\n",
    "\n",
    "# Counter-balances for methods\n",
    "counter_balances_dict = dict()\n",
    "\n",
    "# Numbers of good users for methods\n",
    "good_user_cnt_dict = dict()\n",
    "good_user_cnt = 0\n",
    "bad_user_cnt = 0\n",
    "users_reaction_times = []\n",
    "user_dict = dict()\n",
    "\n",
    "for file in files:\n",
    "    reaction_time_correct_cnt_dict = dict()\n",
    "    users_reaction_time = []\n",
    "    users_val_incorrect_trials = []\n",
    "    counter_balances = []\n",
    "    user_cnt = 0\n",
    "    df = pd.read_csv(file)\n",
    "#     reaction_time = (df['Reaction Time'].sum()/1000)/60 # in minutes\n",
    "    for index, row in df.iterrows():\n",
    "        \n",
    "        # Start the spreadsheet for a user\n",
    "        if row['Event Index'] != 'END OF FILE' and int(row['Event Index']) == 1:\n",
    "            user_cnt += 1\n",
    "#             print('-------- STATISTICS FOR USER NUMBER {} --------'.format(user_cnt))\n",
    "            reaction_time = 0\n",
    "            reaction_times = []\n",
    "\n",
    "            val_correct = 0\n",
    "            val_incorrect = 0\n",
    "            val_incorrect_trials = []\n",
    "            val_trial_cnt = 0\n",
    "            \n",
    "            test_reaction_time = 0\n",
    "            val_reaction_time = 0\n",
    "\n",
    "            test_trial_answers = {}\n",
    "\n",
    "            task_name = row['Task Name']\n",
    "#             print(task_name)\n",
    "            method = task_name.split('_')[1]\n",
    "            if method == 'Confidence':\n",
    "                method = 'Conf'\n",
    "\n",
    "            task = task_name.split('_')[0]\n",
    "            counter_balance = int(row[exp_hierarchy[1][method]])\n",
    "# #             print(task_name, method, counter_balance)\n",
    "#             counter_balances.append(counter_balance)\n",
    "\n",
    "            public_id = row['Participant Public ID']\n",
    "    \n",
    "            if task not in user_dict:\n",
    "                user_dict[task] = dict()\n",
    "                    \n",
    "            if method not in user_dict[task]:\n",
    "                user_dict[task][method] = dict()\n",
    "            if public_id not in user_dict[task][method]:\n",
    "                user_dict[task][method][public_id] = dict()\n",
    "                user_dict[task][method][public_id]['Trials'] = dict()\n",
    "                user_dict[task][method][public_id]['Prior Knowledge'] = dict()\n",
    "                user_dict[task][method][public_id]['Counter balance'] = counter_balance\n",
    "                user_dict[task][method][public_id]['Known'] = 0\n",
    "                user_dict[task][method][public_id]['Unknown'] = 0\n",
    "        \n",
    "        trial_time = row['Reaction Time']\n",
    "        \n",
    "#         # Get the reaction time for each screen\n",
    "#         if (isinstance(trial_time, str) and is_float(trial_time)) or (isinstance(trial_time, float) and not math.isnan(trial_time)):\n",
    "#             reaction_time += float(trial_time)\n",
    "#             reaction_times.append(float(trial_time))\n",
    "            \n",
    "        # Check users' responses in validation\n",
    "        if row['display'] == 'Validation' and row['Screen Name'] == 'Screen 3':\n",
    "            val_trial_cnt += 1\n",
    "            if row['Correct'] == 1:\n",
    "                val_correct += 1\n",
    "            elif row['Incorrect'] == 1:\n",
    "                val_incorrect += 1\n",
    "                val_incorrect_trials.append(val_trial_cnt)\n",
    "            else:\n",
    "                raise ValueError(\"Wrong value!\")\n",
    "                \n",
    "            user_dict[task][method][public_id]['Validation Correct'] = val_correct\n",
    "            user_dict[task][method][public_id]['Validation Incorrect'] = val_incorrect\n",
    "            user_dict[task][method][public_id]['Incorrect Validation Trials'] = val_incorrect_trials\n",
    "            \n",
    "        elif row['display'] == 'Trial' and row['Screen Name'] == 'Screen 1':\n",
    "            prior_knowledge = row['Response']\n",
    "                \n",
    "        # Check users' responses in test\n",
    "        elif row['display'] == 'Trial' and row['Screen Name'] == 'Screen 3':\n",
    "            file_name = row['file_name' + str(counter_balance)]\n",
    "            file_name = (file_name.split('.jpeg')[0]).split('_')[:-1]\n",
    "            file_name = '_'.join(file_name) + '.jpeg'\n",
    "            test_trial_answers[file_name] = row['Response']\n",
    "            \n",
    "            user_dict[task][method][public_id]['Trials'][file_name] = row['Response']\n",
    "            user_dict[task][method][public_id]['Prior Knowledge'][file_name] = prior_knowledge\n",
    "        \n",
    "        # Time from instructions -> the end of Validation\n",
    "        if row['display'] != 'Trial':\n",
    "            if (isinstance(row['Reaction Time'], str) and is_float(row['Reaction Time'])) or (isinstance(row['Reaction Time'], float) and not math.isnan(row['Reaction Time'])):\n",
    "                val_reaction_time += float(row['Reaction Time'])\n",
    "        \n",
    "        # End the spreadsheet for a user\n",
    "        if row['Trial Number'] == 'END TASK':\n",
    "            reaction_time = float(row['Reaction Time'])\n",
    "            \n",
    "            # Validation and Test are combined\n",
    "            user_dict[task][method][public_id]['Validation Reaction Time'] = val_reaction_time\n",
    "            user_dict[task][method][public_id]['Test Reaction Time'] = reaction_time - test_reaction_time\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User_dict\n",
    "- Natural\n",
    "    - Methods\n",
    "        - User_IDs\n",
    "            - ['Trials', '\n",
    "            - Counter balance', \n",
    "            - 'Validation Correct', ' # The number of correct validation trials\n",
    "            - Validation Incorrect', ' # The number of incorrect validation trials\n",
    "            - Incorrect Validation Trials', ' # The IDs of incorrect validation trials\n",
    "            - Validation Reaction Time', '#  # Time from instructions -> the end of Validation\n",
    "            - Test Reaction Time', ' # Time taken for 30 test trials\n",
    "            - Quality']) # This user qualified the validation or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(user_dict['Natural']['SOD'].keys())\n",
    "# print(len(user_dict['Natural']['Conf']))\n",
    "test_correct_trials_dict = dict()\n",
    "\n",
    "# Initialize the dictionary for users' responses (answer)\n",
    "for task in tasks:\n",
    "    final_result[task] = dict()\n",
    "    trial_cnt[task] = dict()\n",
    "    for method in methods:\n",
    "        final_result[task][method] = dict()\n",
    "        trial_cnt[task][method] = dict()\n",
    "        for bin_image in bin_images:\n",
    "            final_result[task][method][bin_image] = 0\n",
    "            trial_cnt[task][method][bin_image] = 0\n",
    "\n",
    "task = exp\n",
    "for method in methods:\n",
    "    print(method)\n",
    "    good_user_cnt = 0\n",
    "    hacking_user_cnt = 0\n",
    "    correct_cnt_list = []\n",
    "    \n",
    "    if method not in test_correct_trials_dict:\n",
    "        test_correct_trials_dict[method] = dict()\n",
    "    for user_id in user_dict[task][method].keys():\n",
    "#         print(user_id)\n",
    "#         print(user_dict[task][method][user_id]['Known'])\n",
    "        \n",
    "        if user_dict[task][method][user_id]['Validation Correct'] >= threshold:\n",
    "            good_user_cnt += 1\n",
    "            user_dict[task][method][user_id]['Quality'] = 'Good'\n",
    "        else:\n",
    "            user_dict[task][method][user_id]['Quality'] = 'Bad'\n",
    "#             if user_dict[task][method][user_id]['Test Reaction Time'] < 1*user_dict[task][method][user_id]['Validation Reaction Time']:\n",
    "#                 hacking_user_cnt += 1\n",
    "            continue\n",
    "        \n",
    "        trials = user_dict[task][method][user_id]['Trials']\n",
    "        \n",
    "        # Initialize: The correct times a trial gets {trial_name: times}\n",
    "        for trial_key in trials.keys():\n",
    "            if trial_key not in test_correct_trials_dict[method]:\n",
    "                test_correct_trials_dict[method][trial_key] = dict()\n",
    "                test_correct_trials_dict[method][trial_key]['Count'] = 0\n",
    "                test_correct_trials_dict[method][trial_key]['Counter balance'] = list()\n",
    "            test_correct_trials_dict[method][trial_key]['Counter balance'].append(user_dict[task][method][user_id]['Counter balance'])\n",
    "                \n",
    "        if task == 'Natural':\n",
    "            task_list = ['Natural', 'Adversarial_Nat']\n",
    "        elif task == 'Dog':\n",
    "            task_list = ['Dog', 'Adversarial_Dog']\n",
    "            \n",
    "        # Record the number of correct answers of a user for every bin\n",
    "        for sub_task in task_list: \n",
    "            user_dict[task][method][user_id][sub_task] = dict()\n",
    "            exp_gt = ground_truth[sub_task]\n",
    "\n",
    "            for CORRECT_BIN_IMAGES in [CORRECT_BIN1_IMAGES, CORRECT_BIN2_IMAGES, CORRECT_BIN3_IMAGES]:\n",
    "                shared_items = {k: exp_gt[CORRECT_BIN_IMAGES][k] for k in exp_gt[CORRECT_BIN_IMAGES] if k in trials and exp_gt[CORRECT_BIN_IMAGES][k] == trials[k]}\n",
    "                user_dict[task][method][user_id][sub_task][CORRECT_BIN_IMAGES] = len(shared_items)\n",
    "                \n",
    "#                 print(len(shared_items))\n",
    "                # Get: The correct times a trial gets {trial_name: times}\n",
    "                for key in shared_items.keys():\n",
    "                    test_correct_trials_dict[method][key]['Count'] += 1\n",
    "                    if user_dict[task][method][user_id]['Prior Knowledge'][key] == 'Yes':\n",
    "                        user_dict[task][method][user_id]['Known'] += 1\n",
    "                    else:\n",
    "                        user_dict[task][method][user_id]['Unknown'] += 1\n",
    "                \n",
    "                # Get: The prediction of AI on this image\n",
    "                for key in exp_gt[CORRECT_BIN_IMAGES].keys():\n",
    "                    if key in test_correct_trials_dict[method]:\n",
    "                        test_correct_trials_dict[method][key]['Prediction'] = 'Correct'\n",
    "                \n",
    "                trial_cnt[sub_task][method][CORRECT_BIN_IMAGES] += len(set.intersection(set(ground_truth[sub_task][CORRECT_BIN_IMAGES].keys()), set(trials.keys())))\n",
    "                \n",
    "            for WRONG_BIN_IMAGES in [WRONG_BIN1_IMAGES, WRONG_BIN2_IMAGES, WRONG_BIN3_IMAGES]:\n",
    "                shared_items = {k: exp_gt[WRONG_BIN_IMAGES][k] for k in exp_gt[WRONG_BIN_IMAGES] if k in trials and exp_gt[WRONG_BIN_IMAGES][k] == trials[k]}\n",
    "                user_dict[task][method][user_id][sub_task][WRONG_BIN_IMAGES] = len(shared_items)\n",
    "#                 print(len(shared_items))\n",
    "                # Get: The correct times a trial gets {trial_name: times}                    \n",
    "                for key in shared_items.keys():\n",
    "                    test_correct_trials_dict[method][key]['Count'] += 1\n",
    "                    if user_dict[task][method][user_id]['Prior Knowledge'][key] == 'Yes':\n",
    "                        user_dict[task][method][user_id]['Known'] += 1\n",
    "                    else:\n",
    "                        user_dict[task][method][user_id]['Unknown'] += 1\n",
    "                    \n",
    "                # Get: The prediction of AI on this image\n",
    "                for key in exp_gt[WRONG_BIN_IMAGES].keys():\n",
    "                    if key in test_correct_trials_dict[method]:\n",
    "                        test_correct_trials_dict[method][key]['Prediction'] = 'Wrong'\n",
    "\n",
    "                trial_cnt[sub_task][method][WRONG_BIN_IMAGES] += len(set.intersection(set(ground_truth[sub_task][WRONG_BIN_IMAGES].keys()), set(trials.keys())))\n",
    "#             print(user_dict[task][method][user_id]['Known'])\n",
    "\n",
    "        print('Method {}: user_id: {} Validation scores: {}'.format(method, user_id, user_dict[task][method][user_id]['Validation Correct']))\n",
    "        print('Correct count = {}'.format(sum(user_dict[exp][method][user_id][tasks[0]].values()) + sum(user_dict[exp][method][user_id][tasks[1]].values())))\n",
    "        correct_cnt = sum(user_dict[exp][method][user_id][tasks[0]].values()) + sum(user_dict[exp][method][user_id][tasks[1]].values())\n",
    "        correct_cnt_list.append((correct_cnt/30)*100)\n",
    "        \n",
    "    print('{}/{} good users of {}'.format(good_user_cnt, len(user_dict[task][method]), method))\n",
    "    print('std_dev = {}'.format(statistics.stdev(correct_cnt_list)))\n",
    "    print('mean = {}'.format(statistics.mean(correct_cnt_list)))\n",
    "#     print('{}/{} hacking users of {}'.format(hacking_user_cnt, len(user_dict[task][method]), method))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1\n",
    "acc_dict = dict()\n",
    "print('Q1: Are heatmaps more effective than 3-NNs in improving AI+human accuracy in classifying random ImageNet images?')\n",
    "for task in tasks:\n",
    "    acc_dict[task] = dict()\n",
    "    print(\"In {} images:\".format(task.split('_')[0]))\n",
    "    accuracies = []\n",
    "    best_acc = 0\n",
    "    best_method = ''\n",
    "    for method in methods:\n",
    "        correct_cnt = 0\n",
    "        known_crt_cnt = 0\n",
    "        unknown_crt_cnt = 0\n",
    "        for user_id in user_dict[exp][method].keys():\n",
    "            if user_dict[exp][method][user_id]['Quality'] == 'Good':\n",
    "                correct_cnt += sum(user_dict[exp][method][user_id][task].values())\n",
    "                known_crt_cnt += user_dict[exp][method][user_id]['Known']\n",
    "                unknown_crt_cnt += user_dict[exp][method][user_id]['Unknown']\n",
    "\n",
    "#                 print(sum(user_dict[exp][method][user_id][task].values()), user_dict[exp][method][user_id]['Known'], user_dict[exp][method][user_id]['Unknown'])\n",
    "\n",
    "        total_cnt = sum(trial_cnt[task][method].values())\n",
    "#         print(\"Correct count: {} on Total count: {}\".format(correct_cnt, total_cnt))\n",
    "        print(correct_cnt, total_cnt)\n",
    "        if total_cnt:\n",
    "            acc = correct_cnt*100/total_cnt\n",
    "            acc_dict[task][method] = round(acc,2) # limit to 2 decimals\n",
    "            accuracies.append(acc)\n",
    "            print(\"Task: {} | Method: {} | Accuracy: {:.2f}%\".format(task, method, correct_cnt*100/total_cnt))\n",
    "            if acc > best_acc:\n",
    "                best_acc = acc\n",
    "                best_method = method\n",
    "                \n",
    "    print(\"Answer: The best method is {} with an accuracy of {:.2f}%\".format(best_method, best_acc))\n",
    "    print(\"Answer: Mean accuracy is {:.2f}%\".format(sum(accuracies)/len(accuracies)))\n",
    "    \n",
    "print(acc_dict)\n",
    "\n",
    "known_crt_acc = []\n",
    "unknown_crt_acc = []\n",
    "for method in methods:\n",
    "    correct_cnt = 0\n",
    "    known_crt_cnt = 0\n",
    "    unknown_crt_cnt = 0\n",
    "    total_known_cnt = 0\n",
    "    total_unknown_cnt = 0\n",
    "    for user_id in user_dict[exp][method].keys():\n",
    "        if user_dict[exp][method][user_id]['Quality'] == 'Good':\n",
    "            known_crt_cnt += user_dict[exp][method][user_id]['Known']\n",
    "            unknown_crt_cnt += user_dict[exp][method][user_id]['Unknown']\n",
    "            total_known_cnt += sum(value == 'Yes' for value in user_dict[exp][method][user_id]['Prior Knowledge'].values())\n",
    "            total_unknown_cnt += sum(value == 'No' for value in user_dict[exp][method][user_id]['Prior Knowledge'].values())\n",
    "    print(known_crt_cnt, total_known_cnt, unknown_crt_cnt, total_unknown_cnt)\n",
    "    known_crt_acc.append(known_crt_cnt*100/total_known_cnt)\n",
    "    unknown_crt_acc.append(unknown_crt_cnt*100/total_unknown_cnt)\n",
    "    print(\"On Known images: Method: {} | Accuracy: {:.2f}%\".format(method, known_crt_cnt*100/total_known_cnt))\n",
    "    print(\"On Unknown images: Method: {} | Accuracy: {:.2f}%\".format(method, unknown_crt_cnt*100/total_unknown_cnt))\n",
    "                           \n",
    "print(\"Answer: Known Mean accuracy is {:.2f}%\".format(sum(known_crt_acc)/len(known_crt_acc)))\n",
    "print(\"Answer: Unnown Mean accuracy is {:.2f}%\".format(sum(unknown_crt_acc)/len(unknown_crt_acc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2\n",
    "print('Q2: Are heatmaps more effective than 3-NNs in improving AI+human accuracy in classifying natural ImageNet images that are harder or easier to AIs?')\n",
    "print('-------------- RESULT OF EASY IMAGES --------------')\n",
    "for task in tasks:\n",
    "    print(\"In {} images:\".format(task.split('_')[0]))\n",
    "    accuracies = []\n",
    "    best_acc = 0\n",
    "    best_method = ''\n",
    "    for method in methods:\n",
    "        correct_cnt = 0\n",
    "        total_cnt = 0\n",
    "        for user_id in user_dict[exp][method].keys():\n",
    "            if user_dict[exp][method][user_id]['Quality'] == 'Good':\n",
    "                for easy_bin in [CORRECT_BIN3_IMAGES]:\n",
    "                    correct_cnt += user_dict[exp][method][user_id][task][easy_bin]\n",
    "        for easy_bin in easy_bins:            \n",
    "#             total_cnt += trial_cnt[task][method][easy_bin]\n",
    "            total_cnt = trial_cnt[task][method][CORRECT_BIN3_IMAGES]\n",
    "        print(correct_cnt, total_cnt)\n",
    "                    \n",
    "#         print(\"Correct count: {} on Total count: {}\".format(correct_cnt, total_cnt))\n",
    "        if total_cnt:\n",
    "            acc = correct_cnt*100/total_cnt\n",
    "            accuracies.append(acc)\n",
    "            print(\"Task: {} | Method: {} | Accuracy: {:.2f}%\".format(task, method, acc))\n",
    "            if acc > best_acc:\n",
    "                best_acc = acc\n",
    "                best_method = method\n",
    "#     print(\"Answer: The best method is {} with an accuracy of {:.2f}%\".format(best_method, best_acc))\n",
    "#     print(\"Answer: Mean accuracy is {:.2f}%\".format(sum(accuracies)/len(accuracies)))\n",
    "    \n",
    "print('-------------- RESULT OF HARD IMAGES --------------')\n",
    "for task in tasks:\n",
    "    print(\"In {} images:\".format(task.split('_')[0]))\n",
    "    accuracies = []\n",
    "    best_acc = 0\n",
    "    best_method = ''\n",
    "    for method in methods:\n",
    "        correct_cnt = 0\n",
    "        total_cnt = 0\n",
    "        for user_id in user_dict[exp][method].keys():\n",
    "            if user_dict[exp][method][user_id]['Quality'] == 'Good':\n",
    "                for hard_bin in [CORRECT_BIN1_IMAGES]:\n",
    "                    correct_cnt += user_dict[exp][method][user_id][task][hard_bin]\n",
    "        for hard_bin in hard_bins:            \n",
    "#             total_cnt += trial_cnt[task][method][hard_bin]\n",
    "            total_cnt = trial_cnt[task][method][CORRECT_BIN1_IMAGES]\n",
    "                    \n",
    "#         print(\"Correct count: {} on Total count: {}\".format(correct_cnt, total_cnt))\n",
    "        print(correct_cnt, total_cnt)\n",
    "        if total_cnt:\n",
    "            acc = correct_cnt*100/total_cnt\n",
    "            accuracies.append(acc)\n",
    "            print(\"Task: {} | Method: {} | Accuracy: {:.2f}%\".format(task, method, acc))\n",
    "            if acc > best_acc:\n",
    "                best_acc = acc\n",
    "                best_method = method\n",
    "#     print(\"Answer: The best method is {} with an accuracy of {:.2f}%\".format(best_method, best_acc))\n",
    "#     print(\"Answer: Mean accuracy is {:.2f}%\".format(sum(accuracies)/len(accuracies)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3\n",
    "print('Q3: Are heatmaps more effective than 3-NNs in improving AI+human accuracy when AIs are not sure i.e. 50/50?')\n",
    "print('-------------- RESULT OF NORMAL IMAGES --------------')\n",
    "for task in tasks:\n",
    "    print(\"In {} images:\".format(task.split('_')[0]))\n",
    "    accuracies = []\n",
    "    best_acc = 0\n",
    "    best_method = ''\n",
    "    for method in methods:\n",
    "        correct_cnt = 0\n",
    "        total_cnt = 0\n",
    "        for user_id in user_dict[exp][method].keys():\n",
    "            if user_dict[exp][method][user_id]['Quality'] == 'Good':\n",
    "                for norm_bin in [CORRECT_BIN2_IMAGES]:\n",
    "                    correct_cnt += user_dict[exp][method][user_id][task][norm_bin]\n",
    "        for norm_bin in norm_bins:            \n",
    "#             total_cnt += trial_cnt[task][method][norm_bin]\n",
    "            total_cnt = trial_cnt[task][method][CORRECT_BIN2_IMAGES]\n",
    "                    \n",
    "#         print(\"Correct count: {} on Total count: {}\".format(correct_cnt, total_cnt))\n",
    "        print(correct_cnt, total_cnt)\n",
    "        if total_cnt:\n",
    "            acc = correct_cnt*100/total_cnt\n",
    "            accuracies.append(acc)\n",
    "            print(\"Task: {} | Method: {} | Accuracy: {:.2f}%\".format(task, method, acc))\n",
    "            if  acc > best_acc:\n",
    "                best_acc = acc\n",
    "                best_method = method\n",
    "#     print(\"Answer: The best method is {} with an accuracy of {:.2f}%\".format(best_method, best_acc))\n",
    "#     print(\"Answer: Mean accuracy is {:.2f}%\".format(sum(accuracies)/len(accuracies)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Q4\n",
    "print('Q4: Which methods help humans the most when AI is correct or wrong?')\n",
    "print('-------------- RESULT OF CORRECT IMAGES --------------')\n",
    "for task in tasks:\n",
    "    print(\"In {} images:\".format(task.split('_')[0]))\n",
    "    best_acc = 0\n",
    "    best_method = ''\n",
    "    for method in methods:\n",
    "        correct_cnt = 0\n",
    "        total_cnt = 0\n",
    "        for user_id in user_dict[exp][method].keys():\n",
    "            if user_dict[exp][method][user_id]['Quality'] == 'Good':\n",
    "                for CORRECT_BIN_IMAGES in [CORRECT_BIN1_IMAGES, CORRECT_BIN2_IMAGES, CORRECT_BIN3_IMAGES]:\n",
    "                    correct_cnt += user_dict[exp][method][user_id][task][CORRECT_BIN_IMAGES]\n",
    "        for CORRECT_BIN_IMAGES in [CORRECT_BIN1_IMAGES, CORRECT_BIN2_IMAGES, CORRECT_BIN3_IMAGES]:     \n",
    "            total_cnt += trial_cnt[task][method][CORRECT_BIN_IMAGES]\n",
    "                    \n",
    "#         print(\"Correct count: {} on Total count: {}\".format(correct_cnt, total_cnt))\n",
    "        if total_cnt:\n",
    "            print(\"Task: {} | Method: {} | Accuracy: {:.2f}%\".format(task, method, correct_cnt*100/total_cnt))\n",
    "            if correct_cnt*100/total_cnt > best_acc:\n",
    "                best_acc = correct_cnt*100/total_cnt\n",
    "                best_method = method\n",
    "    \n",
    "    print(\"Answer: The best method is {} with an accuracy of {:.2f}%\".format(best_method, best_acc))\n",
    "    \n",
    "print('-------------- RESULT OF WRONG IMAGES --------------')\n",
    "for task in tasks:\n",
    "    print(\"In {} images:\".format(task.split('_')[0]))\n",
    "    best_acc = 0\n",
    "    best_method = ''\n",
    "    for method in methods:\n",
    "        correct_cnt = 0\n",
    "        total_cnt = 0\n",
    "        for user_id in user_dict[exp][method].keys():\n",
    "            if user_dict[exp][method][user_id]['Quality'] == 'Good':\n",
    "                for WRONG_BIN_IMAGES in [WRONG_BIN1_IMAGES, WRONG_BIN2_IMAGES, WRONG_BIN3_IMAGES]:\n",
    "                    correct_cnt += user_dict[exp][method][user_id][task][WRONG_BIN_IMAGES]\n",
    "        for WRONG_BIN_IMAGES in [WRONG_BIN1_IMAGES, WRONG_BIN2_IMAGES, WRONG_BIN3_IMAGES]:   \n",
    "            total_cnt += trial_cnt[task][method][WRONG_BIN_IMAGES]\n",
    "                    \n",
    "#         print(\"Correct count: {} on Total count: {}\".format(correct_cnt, total_cnt))\n",
    "        if total_cnt:\n",
    "            print(\"Task: {} | Method: {} | Accuracy: {:.2f}%\".format(task, method, correct_cnt*100/total_cnt))\n",
    "            if correct_cnt*100/total_cnt > best_acc:\n",
    "                best_acc = correct_cnt*100/total_cnt\n",
    "                best_method = method\n",
    "    \n",
    "    print(\"Answer: The best method is {} with an accuracy of {:.2f}%\".format(best_method, best_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q?\n",
    "print('Q?: Test time vs accuracy?')\n",
    "time_vs_accuracy = dict()\n",
    "time_vs_accuracy[exp] = dict()\n",
    "for method in methods:\n",
    "    time_vs_accuracy[exp][method] = dict()\n",
    "    for user_id in user_dict[exp][method].keys():\n",
    "        correct_cnt = 0\n",
    "        if user_dict[exp][method][user_id]['Quality'] == 'Good':\n",
    "            test_time = user_dict[exp][method][user_id]['Test Reaction Time']/60/1000\n",
    "            for task in tasks:\n",
    "                correct_cnt += sum(user_dict[exp][method][user_id][task].values())\n",
    "            time_vs_accuracy[exp][method][test_time] = correct_cnt\n",
    "            \n",
    "    time_vs_accuracy[exp][method] = {k: time_vs_accuracy[exp][method][k] for k in sorted(time_vs_accuracy[exp][method])}\n",
    "    \n",
    "    print(np.corrcoef(list(time_vs_accuracy[exp][method].keys()), list(time_vs_accuracy[exp][method].values())))\n",
    "    ax = plt.figure().gca()\n",
    "    ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    ax.yaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    plt.scatter(time_vs_accuracy[exp][method].keys(), time_vs_accuracy[exp][method].values())\n",
    "    plt.title('Task: {} | Method: {}'.format(exp, method))\n",
    "    plt.ylabel('Correct answers')\n",
    "    plt.xlabel('Minutes')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "# print(time_vs_accuracy)\n",
    "for method in methods:\n",
    "    print(method)\n",
    "    print(sum(time_vs_accuracy[exp][method].keys())/len(time_vs_accuracy[exp][method].keys()))\n",
    "#     print(len(time_vs_accuracy[exp][method].keys()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
